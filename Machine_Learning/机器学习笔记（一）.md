---
title: 《机器学习实战》《西瓜书》笔记（一）
date: 2019-11-26 15:00
categories: Machine_Learning
tags: ML
---

# 《机器学习实战》《西瓜书》笔记（一）
## 机器学习的相关概念
我们要做的其实是让机器他有自己学习的能力，也就我们要做的应该`machine learning`的方向。讲的比较拟人化一点，所谓`machine learning`的方向，就是你就写段程序，然后让机器人变得了很聪明，他就能够有学习的能力。接下来，你就像教一个婴儿、教一个小孩一样的教他，你并不是写程序让他做到这件事，你是写程序让它具有学习的能力。然后接下来，你就可以用像教小孩的方式告诉它。假设你要叫他学会做语音辨识，你就告诉它这段声音是`“Hi”`，这段声音就是`“How are you”`，这段声音是`“Good bye”`。希望接下来它就学会了，你给它一个新的声音，它就可以帮你产生语音辨识的结果。
**用数学的语义去理解，机器需要一个函数对输入进行自主判断输出，以完成回归预测、分类、聚类等实际任务**

- **监督学习**
从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括**回归分析和统计分类**。
- **无监督学习**
与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有**生成对抗网络（GAN）、聚类**
- **半监督学习**
介于监督学习与无监督学习之间
- **增强学习机器**
为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的

## 开发机器学习应用程序的主要步骤
- 收集数据
- 准备输入数据（Python语言）
- 分析输入数据（数据处理、降维等方法）
- 训练算法（无监督学习不需要训练算法）
- 测试算法
- 执行算法

## 机器学习的基本术语
这组记录的集合称为一个 **"数据集" (data set)**
其中每条记录是关于一个事件或对象(这里是一个西瓜)的描述，称为一个 **"示例" (instance) 或"样本" (sample)**
反映事件或对象在某方面的表现或性质的事项，例如"色泽" "根蒂" "敲声"，称为")副主" (attribute) 或 **"特征"**(feature)属性上的取值，例如"青绿" "乌黑"，称为")副主值" (attribute va1ue)
属性张成的空间称为"属性空间" (attribute space)"样本空间" (samp1e space)或"输入空间"
例如我们把"色泽" "根蒂" "敲声"作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此我们也把…个示例称为一个 **"特征向量" (feature vector)**.
`eg:`
$D = \{x_1,x_2,....x_m\} $ 样本包含`m`个实例
$x_i = \{x_{i1}.....x_{id}\}$ 是`d`维样本空间的一个向量
((色泽:青绿;根蒂二蜷缩; 敲声=浊响)，好瓜)" .这里关于示例结果的信息，例如"好瓜"，称为 **"标记" (labe1)**; 拥有了标记信息的示例，则称为"样例" (examp1e).

## 假设空间
归纳是从特殊到一般的“泛化”过程，演绎是从一般到特殊的“特化”过程。 
学习的目的是“泛化”，即通过对训练集中瓜的学习已获得对没见过的瓜进行判断的能力。 
学习过程看作一个在所以假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确实，假设空间及其规模大小就却确定了。 

我们用 m 表示这 个假设.这样，若"色泽" "根蒂" "敲声"分别有3、 2、 2 种可能取值，则我 们面临的假设空间规模大小为 4 x 3 x 3 + 1 = 37.

## 归纳偏好
现在有三个与训练集一致的假设，但与他们对应的模型在面临新样本的时候，却会产生不同的输出。根据仅有的训练样本无法判断三个假设中哪个“更好”。对于一个具体的学习算法而言，它必须要产生一个模型，这时，学习算法本身的“偏好”起到关键左右。例如，若算法喜欢“尽可能特殊”的模型，则会有相应的模型产生。机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。 
任何一个有效的机器学习算法必有其归纳偏好，否则产生的模型每次在进行预测时随机抽选训练集上的等效假设，学得模型结果不一，显然没有意义。

归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进 行选择的启发式或"价值观"那么，有没有一般性的原则来引导算法确立 "正确的"偏好呢? "奥卡姆剃刀" (Occam's razor)是一种常用的、自然科学 研究中最基本的原则，即"若有多个假设与观察一致，则选最简单的那个"如 果采用这个原则，并且假设我们认为"更平滑"意味着"更简单" (例如曲线 A 更易于描述，其方程式是 $y = x2+ 6x + 1$ ，而曲线 B 则要复杂得多)，则在 图1.3 中我们会自然地偏好"平滑"的曲线 A.

## 生成式模型与判别式模型
- 产生式模型
从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。

- 判别式模型
寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。

**区别：**


假设有样本输入值（或者观察值）x，类别标签（或者输出值）y

判别式模型评估对象是最大化条件概率p(y|x)并直接对其建模，

生成式模型评估对象是最大化联合概率p(x,y)并对其建模。

其实两者的评估目标都是要得到最终的类别标签Y， 而Y=argmax p(y|x)，不同的是判别式模型直接通过解在满足训练样本分布下的最优化问题得到模型参数，主要用到拉格朗日乘算法、梯度下降法，常见的判别式模型如最大熵模型、CRF、LR、SVM等；

而生成式模型先经过贝叶斯转换成Y = argmax p(y|x) = argmax p(x|y)*p(y)，然后分别学习p(y)和p(x|y)的概率分布，主要通过极大似然估计的方法学习参数，如NGram、HMM、Naive Bayes。

**优缺点：**

- 生成模型：

优点：
1）实际上带的信息要比判别模型丰富，研究单类问题比判别模型灵活性强
2）模型可以通过增量学习得到
3）生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。

缺点：
1）学习过程比较复杂。
2）实践中多数情况下判别模型效果更好。

- 判别模型：

优点：
1）分类边界更灵活，比使用纯概率方法或生产模型得到的更高级.
2）准确率往往较生成模型高。
3）不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。

缺点：
1）不能反映训练数据本身的特性。